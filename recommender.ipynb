{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OFFER RECOMMENDER PROTOTYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a prototype for an offer recommender which:\n",
    "\n",
    "- Trains a model using the data in the skills data set.\n",
    "- Uses the trained model to recommend offers from the offers data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files containing the data sets and other subjective parameters:\n",
    "\n",
    "**NOTE**: To speed up the process, the `NUMBER_OF_CATEGORIES` and the `NUMBER_OF_CLUSTERS` parameters can be reduced. This, however, lead to worse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_SKILLS = \"data/candidatetest_df_off_sk.csv\"\n",
    "FILE_OFFERS = \"data/candidatetest_df_off_fixed.csv\"\n",
    "PATTERNS_BEGINNING = [\" \", \"+\", \"?\", \"#\", \"$\", \"-\", \"(\", \"*\"]\n",
    "PATTERNS_END = [\" \", \")\"]\n",
    "NUMBER_OF_CATEGORIES = 100\n",
    "NUMBER_OF_CLUSTERS = 200\n",
    "NUMBER_OF_SIMILAR_OFFERS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadOffers(file):\n",
    "    '''\n",
    "    Read the file containing the fixed offers data set.\n",
    "    Args:\n",
    "    - file: File with the fixed offers data set.\n",
    "    Returns:\n",
    "    - Pandas dataframe with tidy offers data set.\n",
    "    '''\n",
    "    data = pd.read_csv(file, delimiter = \"|\", encoding = \"latin1\", dtype = {\"id\": str})\n",
    "    data = data.loc[data[\"requirement\"] != \"#Â¿NOMBRE?\", :]\n",
    "    data[\"studies\"] = [str(s).replace(\";\", \"\") for s in data[\"studies\"]]\n",
    "    return data\n",
    "\n",
    "def ReadSkills(file):\n",
    "    '''\n",
    "    Read the file containing the original skills data set.\n",
    "    Args:\n",
    "    - file: File with the fixed skills data set.\n",
    "    Returns:\n",
    "    - Pandas dataframe with tidy skills data set.\n",
    "    '''\n",
    "    data = pd.read_csv(file, delimiter = \"|\", encoding = \"latin1\")\n",
    "    data = data.rename(columns={\"O_ID\": \"id\", \"OSK_NOMBRE\": \"skill\"})\n",
    "    return data\n",
    "\n",
    "def CleanSkill(skill, patternsBeginning = [], patternsEnd = [], lowerCase = True, stopWords = []):\n",
    "    '''\n",
    "    Perform cleaning to a skill by removing meaningless parts and using only lowercase.\n",
    "    Args:\n",
    "    - skill: String with the skill to clean.\n",
    "    - patternsBeginning: List of patterns to remove from the beginning of the skill.\n",
    "    - patternsEnd: List of patterns to remove from the end of the skill.\n",
    "    - stopWords: List of words to eliminate.\n",
    "    Returns:\n",
    "    - String with the skill clean.\n",
    "    '''\n",
    "    skillClean = str(skill)\n",
    "    skillClean = skillClean.lstrip(\"\".join(patternsBeginning))\n",
    "    skillClean = skillClean.rstrip(\"\".join(patternsEnd))\n",
    "    if lowerCase == True:\n",
    "        skillClean = skillClean.lower()\n",
    "    skillClean = \" \".join([x for x in skillClean.split(\" \") if x not in stopWords])\n",
    "    skillClean = skillClean.replace(\" +\", \" \")\n",
    "    skillClean = skillClean.lstrip()\n",
    "    skillClean = skillClean.rstrip()\n",
    "    return skillClean\n",
    "\n",
    "def CleanSkills(df, patternsBeginning, patternsEnd):\n",
    "    '''\n",
    "    Create a new column to a Pandas dataframe with clean skills.\n",
    "    Args:\n",
    "    - df: Pandas dataframe with a column called \"skill\".\n",
    "    - patternsBeginning: List of patterns to remove from the beginning of the skill.\n",
    "    - patternsEnd: List of patterns to remove from the end of the skill.\n",
    "    Returns:\n",
    "    - Pandas dataframe with a new column called \"skill_clean\".\n",
    "    '''\n",
    "    stopWords = stopwords.words(\"spanish\") + stopwords.words(\"english\")\n",
    "    df[\"skill_clean\"] = df[\"skill\"].apply(lambda x: CleanSkill(x,\n",
    "                                                               patternsBeginning = patternsBeginning,\n",
    "                                                               patternsEnd = patternsEnd,\n",
    "                                                               lowerCase = True,\n",
    "                                                               stopWords = stopWords))\n",
    "    return df\n",
    "\n",
    "def CreateBagOfWords(listOfStrings):\n",
    "    '''\n",
    "    Create a Pandas dataframe containing the individual words present in the skills and\n",
    "    the amount of times they appear.\n",
    "    Args:\n",
    "    - listOfStrings: List with all the skills.\n",
    "    Returns:\n",
    "    - Pandas dataframe with columns \"word\" and \"occurrences\".\n",
    "    '''\n",
    "    bowDict = {}\n",
    "    for string in listOfStrings:\n",
    "        words = string.split(\" \")\n",
    "        for word in words:\n",
    "            word = word.lstrip(\"(\")\n",
    "            word = word.rstrip(\")\")\n",
    "            if word in bowDict.keys():\n",
    "                bowDict[word] = bowDict[word] + 1\n",
    "            else:\n",
    "                bowDict[word] = 1\n",
    "    bowDf = pd.DataFrame.from_dict(bowDict, orient = \"index\", columns = [\"occurrences\"])\n",
    "    bowDf = pd.DataFrame.sort_values(bowDf, \"occurrences\", ascending = False).reset_index()\n",
    "    bowDf = bowDf.rename(columns = {\"index\": \"word\"})\n",
    "    return  bowDf\n",
    "\n",
    "def FeatureEngineering(df, categories):\n",
    "    '''\n",
    "    Create one-hot encoded variables based on the provided categories.\n",
    "    Args:\n",
    "    - df: Pandas dataframe with a column called \"skill_clean\".\n",
    "    - categories: List of words to create the features.\n",
    "    Returns:\n",
    "    - Pandas dataframe with every feature in a column.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    for c in categories:\n",
    "        df.insert(len(df.columns), c, df.skill_clean.apply(lambda x: int(c in x.split(\" \"))))\n",
    "    dfId = df.groupby(\"id\").agg(\"sum\").reset_index()\n",
    "    dfId.iloc[:,1:(NUMBER_OF_CATEGORIES + 1)] = (dfId.iloc[:,1:(NUMBER_OF_CATEGORIES + 1)] != 0) * 1\n",
    "    dfId = dfId.rename(columns = {\"index\": \"id\"})\n",
    "    return dfId\n",
    "\n",
    "def PrepareDataForKMeans(df):\n",
    "    '''\n",
    "    Transforms the Pandas dataframe to a Numpy array to feed the KMeans algorithm.\n",
    "    Args:\n",
    "    - df: Pandas dataframe with all the features needed by the model.\n",
    "    Returns:\n",
    "    - Numpy array prepared to feed the KMeans.\n",
    "    '''\n",
    "    array = np.array(df.drop([\"id\"], axis = 1), dtype = np.float32)\n",
    "    return array\n",
    "\n",
    "def FitModel(array, numberOfClusters):\n",
    "    '''\n",
    "    Creates the KMeans model algorithm.\n",
    "    Args:\n",
    "    - array: Numpy array with values of all the features.\n",
    "    - numberOfClusters: Desired number of clusetrs to divide the observations in.\n",
    "    Returns:\n",
    "    - Dictionary with model algorithm and the parameters needed to use it.\n",
    "    '''\n",
    "    model = KMeans(n_clusters = numberOfClusters)\n",
    "    model = model.fit(array)\n",
    "    return model\n",
    "    \n",
    "def TrainClusteringModel(df, patternsBeginning, patternsEnd, numberOfCategories, numberOfClusters):\n",
    "    '''\n",
    "    Given the skills data set, it akes all the data transformations and trains\n",
    "    the KMeans model algorithm.\n",
    "    Args:\n",
    "    - df: Skills data frame.\n",
    "    - patternsBeginning: List of patterns to remove from the beginning of the skills.\n",
    "    - patternsEnd: List of patterns to remove from the end of the skills.\n",
    "    - numberOfCathegories: Number of features to create the model.\n",
    "    - numberOfClusters: Desired number of clusetrs to divide the observations in.\n",
    "    Returns:\n",
    "    - Dictionary with model algorithm and the parameters needed to use it.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    dfClean = CleanSkills(df, patternsBeginning, patternsEnd)\n",
    "    bowDf = CreateBagOfWords(dfClean[\"skill_clean\"])\n",
    "    categories = bowDf.loc[0:numberOfCategories - 1, \"word\"].tolist()\n",
    "    dfExtended = FeatureEngineering(dfClean, categories)\n",
    "    array = PrepareDataForKMeans(dfExtended)\n",
    "    model = FitModel(array, numberOfClusters)\n",
    "    modelDict = {\"model\": model,\n",
    "                 \"patternsBeginning\": patternsBeginning,\n",
    "                 \"patternsEnd\": patternsEnd,\n",
    "                 \"categories\": categories}\n",
    "    return modelDict\n",
    "\n",
    "def AssignClusterToOffers(offersDf, modelDict):\n",
    "    '''\n",
    "    Process the offers in the Pandas dataframe and assign a cluster to each of them.\n",
    "    Args:\n",
    "    - offersDf: Dataframe with offers.\n",
    "    - modelDict: Dictionary with trained model and parameters used to compute it.\n",
    "    Returns:\n",
    "    - Pandas dataframe of offers with features and cluster.\n",
    "    '''\n",
    "\n",
    "    df = offersDf.copy()\n",
    "    df = df[[\"id\", \"requirement\"]]\n",
    "    df = df.rename(columns={\"requirement\": \"skill\"})\n",
    "    dfClean = CleanSkills(df, modelDict[\"patternsBeginning\"], modelDict[\"patternsEnd\"])\n",
    "    dfExtended = FeatureEngineering(dfClean, modelDict[\"categories\"])\n",
    "    array = PrepareDataForKMeans(dfExtended)\n",
    "    dfExtended[\"cluster\"] = modelDict[\"model\"].predict(array)\n",
    "    return dfExtended\n",
    "\n",
    "def AskForOffer():\n",
    "    '''\n",
    "    Ask to enter the skills for a new offer.\n",
    "    Args:\n",
    "    - Nothing.\n",
    "    Returns:\n",
    "    - String with the entered skills.\n",
    "    '''\n",
    "    skillsInputString = input(\"Add skills of the new offer separated by commas: \")\n",
    "    return(skillsInputString)\n",
    "\n",
    "def AssignClusterToNewOffer(skillsString, modelDict):\n",
    "    '''\n",
    "    Process the new offer to make a prediction and assign a cluster to it.\n",
    "    Args:\n",
    "    - skillsString: Skills string entered by the user.\n",
    "    - modelDict: Dictionary with trained model and parameters used to compute it.\n",
    "    Returns:\n",
    "    - Pandas dataframe of new offer with features and cluster.\n",
    "    '''\n",
    "    skillsInputList = skillsString.split(\", \")\n",
    "    print(\"\")\n",
    "    print(\"Requested skills:\")\n",
    "    for i in skillsInputList:\n",
    "        print(\"- \" + i)\n",
    "    print(\"\")\n",
    "    skillsInputDf = pd.DataFrame({\"id\": 42, \"skill\": skillsInputList})\n",
    "    skillsInputDfClean = CleanSkills(skillsInputDf, modelDict[\"patternsBeginning\"], modelDict[\"patternsEnd\"])\n",
    "    skillsInputDfExtended = FeatureEngineering(skillsInputDfClean, modelDict[\"categories\"])\n",
    "    array = PrepareDataForKMeans(skillsInputDfExtended)\n",
    "    skillsInputDfExtended[\"cluster\"] = modelDict[\"model\"].predict(array)\n",
    "    skillsInputDfExtended\n",
    "    return skillsInputDfExtended    \n",
    "\n",
    "def CalculateDistances(offersDf, offer):\n",
    "    '''\n",
    "    Calculate the distance in the multi-dimensional space of an offer to all offers\n",
    "    in the offers data set.\n",
    "    Args:\n",
    "    - offersDf: Dataframe with offers and all the features.\n",
    "    - offer: Dataframe with the new offer.\n",
    "    Returns:\n",
    "    - Pandas dataframe with distances.\n",
    "    '''\n",
    "    offersDf = offersDf.copy()\n",
    "    categories = [s for s in list(offer.columns) if s not in [\"id\", \"cluster\"]]\n",
    "    offerArray = np.array(offer[categories])[0]\n",
    "    offersDf[\"distance\"] = (offersDf[categories] - offerArray).pow(2).sum(1)\n",
    "    offersDf = pd.DataFrame.sort_values(offersDf, \"distance\").reset_index()\n",
    "    return offersDf\n",
    "    \n",
    "def FindSimilarOffers(offersWithCluster, modelDict, numberOfSimilarOffers):\n",
    "    '''\n",
    "    Ask for skills of a new offer and returns the more similar ones.\n",
    "    Args:\n",
    "    - offersWithCluster: Pandas dataframe with offers, all features and cluster.\n",
    "    - modelDict: Dictionary with trained model and parameters used to compute it.\n",
    "    - numberOfSimilarOffers: Amount of offers to show.\n",
    "    Returns:\n",
    "    - List of the most similar offer ids to the entered one.\n",
    "    '''\n",
    "    skillsInputString = AskForOffer()\n",
    "    newOfferCluster = AssignClusterToNewOffer(skillsInputString, modelDict)\n",
    "    cluster = newOfferCluster[\"cluster\"][0]\n",
    "    offersCluster = offersWithCluster.loc[offersWithCluster[\"cluster\"] == cluster, ]\n",
    "    closerOffers = CalculateDistances(offersCluster, newOfferCluster)\n",
    "    recommendedIds = closerOffers.iloc[0:numberOfSimilarOffers, ][\"id\"]\n",
    "    return recommendedIds\n",
    "\n",
    "def ShowSimilarOffers(df, ids):\n",
    "    '''\n",
    "    Show in the screen the information about the provided offers.\n",
    "    Args:\n",
    "    - df: Dataframe with offers.\n",
    "    - ids: List of offer ids to show.\n",
    "    Returns:\n",
    "    - Nothing.\n",
    "    '''\n",
    "    print(\"------------------------------------------\")\n",
    "    offersShow = df.loc[df[\"id\"].isin(ids), ]\n",
    "    for id in ids:\n",
    "        offersId = offersShow.loc[df[\"id\"] == id, ]\n",
    "        name = offersId[\"name\"].iloc[0]\n",
    "        requirements = offersId[\"requirement\"]\n",
    "        studies = offersId[\"studies\"].iloc[0]\n",
    "        print(\"\")\n",
    "        print(\"Offer name:\")\n",
    "        print(\" \" + name)\n",
    "        print(\"Required skills: \")\n",
    "        for i in requirements:\n",
    "            print(\" \" + str(i))\n",
    "        print(\"Required level of studies:\")\n",
    "        print(\" \" + studies)\n",
    "        print(\"\")\n",
    "        print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = ReadSkills(FILE_SKILLS)\n",
    "offers = ReadOffers(FILE_OFFERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrainClusteringModel(df = skills,\n",
    "                             patternsBeginning = PATTERNS_BEGINNING,\n",
    "                             patternsEnd = PATTERNS_END,\n",
    "                             numberOfCategories = NUMBER_OF_CATEGORIES,\n",
    "                             numberOfClusters = NUMBER_OF_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign a cluster to each of the offers in the pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offersWithCluster = AssignClusterToOffers(offers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar offers to a given one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarOffers = FindSimilarOffers(offersWithCluster, model, NUMBER_OF_SIMILAR_OFFERS)\n",
    "ShowSimilarOffers(offers, similarOffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some offers are never shown.\n",
    "- Some skills are not captured so the model does not find suitable offers.\n",
    "\n",
    "I would have better results if comparing the new offer with those used to train the model. For comparing with offers totally unseed by the training algorithm, results are not bad.\n",
    "\n",
    "Also, training the model with more categories and more clusters gives better results, but it requires more computational capacity and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More things to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Improve model performance.\n",
    "- Do feature engineering to find better variables.\n",
    "- Test results with the unseen offers.\n",
    "- Brainstorming with team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

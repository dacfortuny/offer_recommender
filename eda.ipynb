{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the exploration process that lead to defining the final model for recommending offers. The steps that I have followed are:\n",
    "\n",
    "- Retrieve data from data files.\n",
    "- Basic exploration of the files.\n",
    "- Data cleaning.\n",
    "- Exploration of possible features for the classification model.\n",
    "- First attempt of a K-Means classification model.\n",
    "- Selection of the best parameters for the model.\n",
    "\n",
    "The prototype application of the offer recommender is contained in the notebook called **recommender**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files containing the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_SKILLS = \"data/candidatetest_df_off_sk.csv\"\n",
    "FILE_OFFERS = \"data/candidatetest_df_off_fixed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided offers file (candidatetest_df_off.csv) was not a proper CSV, so I had to perform a process to parse it. The process took several hours, so here I start working with the processed file directly. The notebook called **parse_offers_file** contains the code used to obtain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadOffers(file):\n",
    "    '''\n",
    "    Read the file containing the fixed offers data set.\n",
    "    Args:\n",
    "    - file: File with the fixed offers data set.\n",
    "    Returns:\n",
    "    - Pandas dataframe with tidy offers data set.\n",
    "    '''\n",
    "    data = pd.read_csv(file, delimiter = \"|\", encoding = \"latin1\", dtype = {\"id\": str})\n",
    "    data = data.loc[data[\"requirement\"] != \"#Â¿NOMBRE?\", :]\n",
    "    data[\"studies\"] = [str(s).replace(\";\", \"\") for s in data[\"studies\"]]\n",
    "    return data\n",
    "\n",
    "offers = ReadOffers(FILE_OFFERS)\n",
    "offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadSkills(file):\n",
    "    '''\n",
    "    Read the file containing the original skills data set.\n",
    "    Args:\n",
    "    - file: File with the fixed skills data set.\n",
    "    Returns:\n",
    "    - Pandas dataframe with tidy skills data set.\n",
    "    '''\n",
    "    data = pd.read_csv(file, delimiter = \"|\", encoding = \"latin1\")\n",
    "    data = data.rename(columns={\"O_ID\": \"id\", \"OSK_NOMBRE\": \"skill\"})\n",
    "    return data\n",
    "\n",
    "skills = ReadSkills(FILE_SKILLS)\n",
    "skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offersNum = len(set(offers[\"id\"]))\n",
    "namesNum = len(set(offers[\"name\"]))\n",
    "requirementsNum = len(set(offers[\"requirement\"]))\n",
    "studiesNum = len(set(offers[\"studies\"]))\n",
    "print(\"Ths offers dataset contains:\")\n",
    "print(\"- \"+ format(offersNum, \",\") + \" distinct offers.\")\n",
    "print(\"- \"+ format(namesNum, \",\") + \" distinct names.\")\n",
    "print(\"- \"+ format(requirementsNum, \",\") + \" distinct requirements.\")\n",
    "print(\"- \"+ format(studiesNum, \",\") + \" distinct studies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offersNum = len(set(skills[\"id\"]))\n",
    "skillsNum = len(set(skills[\"skill\"]))\n",
    "print(\"Ths skills dataset contains:\")\n",
    "print(\"- \"+ format(offersNum, \",\") + \" distinct offers.\")\n",
    "print(\"- \"+ format(skillsNum, \",\") + \" distinct skills.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offersIds = list(set(offers.loc[:, \"id\"]))\n",
    "skillsIds = list(set(skills.loc[:, \"id\"]))\n",
    "offers_in_skills_and_offers = sum(pd.Series(skillsIds).isin(offersIds))\n",
    "offers_no_skills_in_offers = sum(~pd.Series(offersIds).isin(skillsIds))\n",
    "offers_in_skills_no_offers = sum(~pd.Series(skillsIds).isin(offersIds))\n",
    "print(\"Offers in both datasets: \" + format(offers_in_skills_and_offers, \",\"))\n",
    "print(\"Offers in the offers dataset but not in the skills dataset: \" + format(offers_no_skills_in_offers, \",\"))\n",
    "print(\"Offers in the skills dataset but not in the offers dataset: \" + format(offers_in_skills_no_offers, \",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compare the data for one random id in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills.loc[skills[\"id\"] == \"31854473284\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers.loc[offers[\"id\"] == \"31854473284\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offers in both files do not seem to match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning offers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I group the studies in subgroups based on profiles of candidates that could be interested in the same kind of offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Different studies:\")\n",
    "set(offers[\"studies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddStudiesType(df):\n",
    "    '''\n",
    "    Add a column to the dataframe containing a broader type of studies.\n",
    "    Args:\n",
    "    - df: Pandas dataframe with a column called \"studies\".\n",
    "    Returns:\n",
    "    - Pandas dataframe with a new column called \"studies_type\".\n",
    "    '''\n",
    "    studiesDict = {\"(Indicar Nivel)\": \"no indicado\",\n",
    "                   \"Alta capacidad de trabajo en equipo y de relacion con otros grupos tecnicos\\t\\t\": \"no indicado\",\n",
    "                   \"Bachillerato\": \"secundaria\",\n",
    "                   \"Ciclo Formativo Gr\": \"fp\",\n",
    "                   \"Ciclo Formativo Grado\": \"fp\",\n",
    "                   \"Ciclo Formativo Grado Medio\": \"fp\",\n",
    "                   \"Ciclo Formativo Grado Superior\": \"fp\",\n",
    "                   \"Diplomatura\": \"universidad\",\n",
    "                   \"Doctorado\": \"posgrado\",\n",
    "                   \"Educacion \": \"no idicado\",\n",
    "                   \"Educacion Secundaria Obligatoria\": \"secundaria\",\n",
    "                   \"Ense?anzas artisticas (regladas)\": \"fp\",\n",
    "                   \"Ense?anzas deportivas (regladas)\": \"fp\",\n",
    "                   \"Formaci\": \"no indicado\",\n",
    "                   \"Formacion Profesion\": \"fp\",\n",
    "                   \"Formacion Profesional Grado Me\": \"fp\",\n",
    "                   \"Formacion Profesional Grado Medio\": \"fp\",\n",
    "                   \"Formacion Profesional Grado S\": \"fp\",\n",
    "                   \"Formacion Profesional Grado Super\": \"fp\",\n",
    "                   \"Formacion Profesional Grado Superior\": \"fp\",\n",
    "                   \"Gra\": \"universidad\",\n",
    "                   \"Grado\": \"universidad\",\n",
    "                   \"Ingenieria Superior\": \"universidad\",\n",
    "                   \"Ingenieria Tecn\": \"universidad\",\n",
    "                   \"Ingenieria Tecnica\": \"universidad\",\n",
    "                   \"Licenciatura\": \"universidad\",\n",
    "                   \"Master\": \"posgrado\",\n",
    "                   \"Otros cursos y formacion no reglada\": \"no indicado\",\n",
    "                   \"Otros titulos\": \"no indicado\",\n",
    "                   \"Postgrado\": \"posgrado\",\n",
    "                   \"Sin estudios\": \"sin estudios\",\n",
    "                   \"nan\": \"no indicado\"}\n",
    "    df[\"studies_type\"] = df[\"studies\"].map(studiesDict)\n",
    "    return df\n",
    "\n",
    "offers = AddStudiesType(offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of offers per type of studies:\")\n",
    "offers[[\"id\", \"studies_type\"]].groupby(\"studies_type\").count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of cleaning skills consists on:\n",
    "- Removing some patterns such as\n",
    "    - Symbols working as a bullet points at the beginning.\n",
    "    - Parentheses.\n",
    "    - Meaningless words such as articles, prepositions, etc.\n",
    "    - Blank spaces.\n",
    "- Using only lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERNS_BEGINNING = [\" \", \"+\", \"?\", \"#\", \"$\", \"-\", \"(\", \"*\"]\n",
    "PATTERNS_END = [\" \", \")\"]\n",
    "STOP_WORDS = stopwords.words(\"spanish\") + stopwords.words(\"english\")\n",
    "\n",
    "def CleanSkill(skill, patternsBeginning = [], patternsEnd = [], lowerCase = True, stopWords = []):\n",
    "    '''\n",
    "    Performe cleaning to a skill by removing meaningless parts and using only lowercase.\n",
    "    Args:\n",
    "    - skill: String with the skill to clean.\n",
    "    - patternsBeginning: List of patterns to remove from the beginning of the skill.\n",
    "    - patternsEnd: List of patterns to remove from the end of the skill.\n",
    "    - stopWords: List of words to eliminate.\n",
    "    Returns:\n",
    "    - String with the skill clean.\n",
    "    '''\n",
    "    skillClean = str(skill)\n",
    "    skillClean = skillClean.lstrip(\"\".join(patternsBeginning))\n",
    "    skillClean = skillClean.rstrip(\"\".join(patternsEnd))\n",
    "    if lowerCase == True:\n",
    "        skillClean = skillClean.lower()\n",
    "    skillClean = \" \".join([x for x in skillClean.split(\" \") if x not in stopWords])\n",
    "    skillClean = skillClean.replace(\" +\", \" \")\n",
    "    skillClean = skillClean.lstrip()\n",
    "    skillClean = skillClean.rstrip()\n",
    "    return(skillClean)\n",
    "\n",
    "def CleanSkills(df, patternsBeginning, patternsEnd, stopWords):\n",
    "    '''\n",
    "    Create a new column to a Pandas dataframe with clean skills.\n",
    "    Args:\n",
    "    - df: Pandas dataframe with a column called \"skill\".\n",
    "    - patternsBeginning: List of patterns to remove from the beginning of the skill.\n",
    "    - patternsEnd: List of patterns to remove from the end of the skill.\n",
    "    - stopWords: List of words to eliminate.\n",
    "    Returns:\n",
    "    - Pandas dataframe with a new column called \"skill_clean\".\n",
    "    '''\n",
    "    df[\"skill_clean\"] = df[\"skill\"].apply(lambda x: CleanSkill(x,\n",
    "                                                               patternsBeginning = patternsBeginning,\n",
    "                                                               patternsEnd = patternsEnd,\n",
    "                                                               lowerCase = True,\n",
    "                                                               stopWords = stopWords))\n",
    "    return df\n",
    "\n",
    "skills = CleanSkills(skills, PATTERNS_BEGINNING, PATTERNS_END, STOP_WORDS)\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanSkillsNum = len(set(skills[\"skill_clean\"]))\n",
    "print(\"Ths skills dataset contains:\")\n",
    "print(\"- \"+ format(skillsNum, \",\") + \" distinct skills.\")\n",
    "print(\"- \"+ format(cleanSkillsNum, \",\") + \" distinct clean skills.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a **bag of words** means having all distinct words that form all the (clean) skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateBagOfWords(listOfStrings):\n",
    "    '''\n",
    "    Create a Pandas dataframe containing the individual words present in the skills and\n",
    "    the amount of times they appear.\n",
    "    Args:\n",
    "    - listOfStrings: List with all the skills.\n",
    "    Returns:\n",
    "    - Pandas dataframe with columns \"word\" and \"occurrences\".\n",
    "    '''\n",
    "    bowDict = {}\n",
    "    for string in listOfStrings:\n",
    "        words = string.split(\" \")\n",
    "        for word in words:\n",
    "            word = word.lstrip(\"(\")\n",
    "            word = word.rstrip(\")\")\n",
    "            if word in bowDict.keys():\n",
    "                bowDict[word] = bowDict[word] + 1\n",
    "            else:\n",
    "                bowDict[word] = 1\n",
    "    bowDf = pd.DataFrame.from_dict(bowDict, orient = \"index\", columns = [\"occurrences\"])\n",
    "    bowDf = pd.DataFrame.sort_values(bowDf, \"occurrences\", ascending = False).reset_index()\n",
    "    bowDf = bowDf.rename(columns = {\"index\": \"word\"})\n",
    "    return(bowDf)\n",
    "\n",
    "bowDf = CreateBagOfWords(skills[\"skill_clean\"])\n",
    "bowDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsNum = len(set(bowDf[\"word\"]))\n",
    "print(\"Ths clean skills contain:\")\n",
    "print(\"- \"+ format(wordsNum, \",\") + \" distinct words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a clustering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a clustering model using the skills dataset. I will group together those offers that require similar skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the data in train and test subsets. I will create the model with the train set and evaluate the best parameters for the model using the test set. After that, in the notebook that contains the prototype I will perform tests with unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestSplit(df, testSize = 0.8):\n",
    "    '''\n",
    "    Split dataset in train and test subsets.\n",
    "    Args:\n",
    "    - df: Pandas dataframe containing the dataset to split.\n",
    "    - testSize: Portion of the ids in the test subset.\n",
    "    Returns:\n",
    "    - Two pandas dataframes with the two subsets.\n",
    "    '''\n",
    "    ids = pd.Series(list(set(df[\"id\"])))\n",
    "    idsLen = len(ids)\n",
    "    np.random.seed(31)\n",
    "    msk = np.random.rand(idsLen) < testSize\n",
    "    trainIds = ids[msk]\n",
    "    trainDf = df[df[\"id\"].isin(trainIds)]\n",
    "    testIds = ids[~msk]\n",
    "    testDf = df[df[\"id\"].isin(testIds)]\n",
    "    return(trainDf.copy(), testDf.copy())\n",
    "\n",
    "skillsTrain, skillsTest = TrainTestSplit(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create one-hot encoded features for the words with more occurrences. They are 1 if the word is present in at least one of the skills of the offer, an 0 otherwise. To show how it works I create features based on 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CATEGORIES = 10\n",
    "\n",
    "categories = bowDf.loc[0:NUMBER_OF_CATEGORIES - 1, \"word\"].tolist()\n",
    "\n",
    "def FeatureEngineering(df, categories):\n",
    "    '''\n",
    "    Create one-hot encoded variables based on the provided categories.\n",
    "    Args:\n",
    "    - df: Pandas dataframe with a column called \"skill_clean\".\n",
    "    - categories: List of words to create the features.\n",
    "    Returns:\n",
    "    - Pandas dataframe with every feature in a column.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    for c in categories:\n",
    "        df.insert(len(df.columns), c, df.skill_clean.apply(lambda x: int(c in x.split(\" \"))))\n",
    "    dfId = df.groupby(\"id\").agg(\"sum\").reset_index()\n",
    "    dfId.iloc[:,1:(NUMBER_OF_CATEGORIES + 1)] = (dfId.iloc[:,1:(NUMBER_OF_CATEGORIES + 1)] != 0) * 1\n",
    "    dfId = dfId.rename(columns = {\"index\": \"id\"})\n",
    "    return dfId\n",
    "\n",
    "modelDf = FeatureEngineering(skillsTrain, categories)\n",
    "modelDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using K-Means (first attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first try a K-Means clustering with K = 50; later I will explore which is the best number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLUSTERS = 50\n",
    "\n",
    "array = np.array(modelDf.drop([\"id\"], axis = 1), dtype = np.float32)\n",
    "model = KMeans(n_clusters = NUMBER_OF_CLUSTERS)\n",
    "model = model.fit(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDf[\"cluster\"] = model.labels_\n",
    "print(\"Number of offers per cluster:\")\n",
    "for n in range(NUMBER_OF_CLUSTERS):\n",
    "    num = len(modelDf.loc[modelDf[\"cluster\"] == n][\"id\"].tolist())\n",
    "    print(\"Cluster: \" + str(n) + \": \" + str(num) + \" offers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER = 20\n",
    "mergeDf = pd.merge(skillsTrain[[\"id\", \"skill\"]], modelDf[[\"id\", \"cluster\"]], on = \"id\")\n",
    "idsCluster = list(set(mergeDf.loc[mergeDf[\"cluster\"] == CLUSTER][\"id\"].tolist()))\n",
    "print(\"Examples of offers in the same cluster:\")\n",
    "for i in idsCluster[0:20]:\n",
    "    print(set(mergeDf.loc[mergeDf[\"id\"] == i][\"skill\"].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the goodness of the clustering, I calculate the mean distance of each offer to the centroid of the cluster it belongs to. I do that in the training set (used to calculate the clusters) and in the training set. The goal of this process is to find the best number of clusters.\n",
    "\n",
    "I will be using 200 categories. This number shoud be tunned in order to find the optimal number of categories.\n",
    "\n",
    "This step takes long time. For this reason, I added the `readFromFile` parameter in the `CalculateScoreForDifferentK` function. When `TRUE`, the resulting scores are read from a cached CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CATEGORIES = 200\n",
    "NUMBER_OF_CLUSTERS_LIST = [10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "def CalculateMeanDistance(df, model):\n",
    "    '''\n",
    "    Calculate the mean distance of each observation to the centroid of the cluster\n",
    "    it belongs to.\n",
    "    Args:\n",
    "    - df: Pandas dataframe with a column called \"cluster\" and columns with features.\n",
    "    - model: KMeans object containing the trained model.\n",
    "    Returns:\n",
    "    - Mean distance.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    centers = model.cluster_centers_\n",
    "    distance = 0\n",
    "    for i in range(len(centers)):\n",
    "        distances = (df.loc[df[\"cluster\"] == i, categories] - np.array(centers[i])).pow(2).sum(1).pow(0.5)\n",
    "        distance = distance + distances.sum()\n",
    "    distance = distance / len(df.index)\n",
    "    return distance\n",
    "\n",
    "def CalculateScoreForDifferentK(df, kList, file = \"scores.csv\", readFromFile = False):\n",
    "    '''\n",
    "    Try different values of K for performing the KMeans and saves results in a CSV file.\n",
    "    Args:\n",
    "    - df: Pandas dataframe over which make the predictions (test).\n",
    "    - KList: List of values of K to test.\n",
    "    - file: File in which write the results or read them if readFromFile is True.\n",
    "    - readFromFile: If True, results are read from file and computations are avoided.\n",
    "    Returns:\n",
    "    - Dataframe with results.\n",
    "    '''\n",
    "    if readFromFile:\n",
    "        scoresDf = pd.read_csv(file)\n",
    "        return scoresDf\n",
    "    modelDfTrain, modelDfTest = TrainTestSplit(df)\n",
    "    arrayTrain = np.array(modelDfTrain.drop([\"id\"], axis = 1), dtype = np.float32)\n",
    "    arrayTest = np.array(modelDfTest.drop([\"id\"], axis = 1), dtype = np.float32)\n",
    "    scoresDf = pd.DataFrame(columns = [\"clusters\", \"score\", \"set\"])\n",
    "    for k in NUMBER_OF_CLUSTERS_LIST:\n",
    "        print(str(k) + \" clusters\")\n",
    "\n",
    "        model = KMeans(n_clusters = k)\n",
    "        model = model.fit(arrayTrain)\n",
    "        modelDfTrain.loc[:, \"cluster\"] = model.labels_\n",
    "\n",
    "        predictions = model.predict(arrayTest)\n",
    "        modelDfTest.loc[:, \"cluster\"] = predictions\n",
    "\n",
    "        scoreTest = CalculateMeanDistance(modelDfTest, model)\n",
    "        scoreTrain = CalculateMeanDistance(modelDfTrain, model)\n",
    "\n",
    "        scoresDf = scoresDf.append({\"clusters\": k,\n",
    "                                    \"score\": scoreTest,\n",
    "                                    \"set\": \"test\"}, ignore_index = True)\n",
    "        scoresDf = scoresDf.append({\"clusters\": k,\n",
    "                                    \"score\": scoreTrain,\n",
    "                                    \"set\": \"train\"}, ignore_index = True)\n",
    "    scoresDf.to_csv(file, index = False)\n",
    "    return scoresDf\n",
    "    \n",
    "categories = bowDf.loc[0:NUMBER_OF_CATEGORIES - 1, \"word\"].tolist()\n",
    "modelDf = FeatureEngineering(skills, categories)\n",
    "scoresDf = CalculateScoreForDifferentK(modelDf, NUMBER_OF_CLUSTERS_LIST, file = \"scores.csv\", readFromFile = True)\n",
    "scoresDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dplot = scoresDf.copy()\n",
    "dplot.set_index(\"clusters\", inplace = True)\n",
    "dplot.groupby(\"set\")[\"score\"].plot(legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dplot.groupby(\"set\")[\"score\"].plot(legend = True, ylim = (0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the larger the number of clusters, the lower the mean distance of observation to the center of its cluster. With this result, I will build a clustering model with 200 clusters. The number of clusters has been selected based on performance and time of execution. I know that more tests should be done to find the best model, including tunning other parameters and trying other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things I would do if I had more time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Improving the feature engineering process using methods to obtain the optimal number of categories.\n",
    "- Inculde data from the offers file to the model.\n",
    "- Create categories by joining similar words.\n",
    "- Use the distance between words instead of whether they are present or not.\n",
    "- Using NLP to join different words with similar meaning.\n",
    "- Trying other algorithms, in particular those in which an observation can fall in more than one cluster.\n",
    "- Understand how the clustering is done in order to gain insights that can help improving the model.\n",
    "- Performing PCA to reduce the number of features and speed up the process.\n",
    "- Run tests using the offers in the offers data set.\n",
    "\n",
    "The **recommender** notebook includes a working prototype consisting on a clustering model created based on the insights\n",
    "presented here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
